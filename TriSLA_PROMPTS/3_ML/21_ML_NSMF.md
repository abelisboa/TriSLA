# 21 ‚Äì Implementa√ß√£o Completa do ML-NSMF  

**TriSLA ‚Äì M√≥dulo de Machine Learning para Previs√£o de Viabilidade de SLAs**

---

## üéØ Objetivo Geral

Implementar o m√≥dulo **ML-NSMF (Machine Learning Network Slice Management Function)** que utiliza t√©cnicas de **Machine Learning** para prever a viabilidade de aceita√ß√£o de SLAs baseado em:

- **M√©tricas hist√≥ricas** do NASP
- **Caracter√≠sticas do NEST** gerado pelo SEM-CSMF
- **Estado atual dos recursos** da infraestrutura
- **Padr√µes de uso** anteriores

O m√≥dulo fornece:
- **Score de viabilidade** (0-1)
- **Explicabilidade (XAI)** das previs√µes
- **Recomenda√ß√µes** de ajuste de requisitos
- **Interface I-03** (Kafka) para comunica√ß√£o com Decision Engine

---

## üìã Requisitos Funcionais

### 1. Coleta de Dados

- Receber **m√©tricas do NASP** (RAN, Transport, Core)
- Receber **NEST** do SEM-CSMF via interface I-02
- Coletar **m√©tricas hist√≥ricas** de slices anteriores
- Armazenar **datasets** para treinamento cont√≠nuo

### 2. Preprocessamento

- **Normaliza√ß√£o** de features
- **Feature engineering** (extra√ß√£o de caracter√≠sticas relevantes)
- **Handling de dados faltantes**
- **Balanceamento** de classes (se necess√°rio)

### 3. Modelo de ML

- **Modelo LSTM ou GRU** para s√©ries temporais
- **Alternativa:** Random Forest ou XGBoost para features est√°ticas
- **Treinamento** com dados hist√≥ricos
- **Valida√ß√£o cruzada** e m√©tricas de avalia√ß√£o
- **Persist√™ncia** do modelo treinado (model.h5 ou pickle)

### 4. Previs√£o

- **Score de viabilidade** (probabilidade de aceita√ß√£o)
- **Threshold** configur√°vel (ex: 0.7)
- **Tempo de resposta** < 500ms

### 5. Explicabilidade (XAI)

- **SHAP values** para explica√ß√£o de features
- **LIME** para explica√ß√£o local
- **Feature importance** ranking
- **Logs explic√°veis** para auditoria

### 6. Interface I-03 (Kafka)

- **Producer** para enviar previs√µes ao Decision Engine
- **Consumer** para receber NESTs do SEM-CSMF
- **Retry logic** para mensagens falhadas
- **Dead letter queue** para mensagens problem√°ticas

---

## üèóÔ∏è Arquitetura do M√≥dulo

```
apps/ml-nsmf/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main.py                 # FastAPI application
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ predictor.py        # Modelo de ML
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preprocessor.py     # Preprocessamento
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ explainer.py        # XAI (SHAP/LIME)
‚îÇ   ‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train.py            # Script de treinamento
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py      # Carregador de dados
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ feature_engineering.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ evaluator.py        # Avalia√ß√£o do modelo
‚îÇ   ‚îú‚îÄ‚îÄ kafka/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ producer.py         # Producer Kafka I-03
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ consumer.py         # Consumer Kafka I-02
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schemas.py          # Schemas Avro/JSON
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ datasets/           # Datasets hist√≥ricos
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models/             # Modelos treinados (model.h5)
‚îÇ   ‚îú‚îÄ‚îÄ observability/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ otlp_exporter.py    # Exportador OTLP
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metrics.py          # M√©tricas Prometheus
‚îÇ   ‚îî‚îÄ‚îÄ config.py               # Configura√ß√µes
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îî‚îÄ‚îÄ training/
‚îú‚îÄ‚îÄ notebooks/                  # Jupyter notebooks para an√°lise
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## üîß Implementa√ß√£o T√©cnica

### 1. Modelo de Machine Learning

**Op√ß√£o 1: LSTM/GRU (S√©ries Temporais)**

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

model = Sequential([
    LSTM(128, return_sequences=True, input_shape=(timesteps, features)),
    Dropout(0.2),
    LSTM(64, return_sequences=False),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Score de viabilidade
])
```

**Op√ß√£o 2: Random Forest / XGBoost (Features Est√°ticas)**

```python
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

model = XGBClassifier(
    n_estimators=100,
    max_depth=10,
    learning_rate=0.1,
    objective='binary:logistic'
)
```

**Escolha baseada em:**
- Se h√° depend√™ncia temporal ‚Üí LSTM/GRU
- Se features s√£o est√°ticas ‚Üí Random Forest/XGBoost

### 2. Features (Inputs)

**Do NEST:**
- `sliceType` (eMBB/URLLC/mMTC)
- `latency_requirement`
- `throughput_requirement`
- `reliability_requirement`
- `coverage_area`

**Do NASP (M√©tricas atuais):**
- `cpu_utilization`
- `memory_utilization`
- `network_bandwidth_available`
- `active_slices_count`
- `prb_utilization` (RAN)

**Hist√≥ricas:**
- `success_rate_last_30_days`
- `average_latency_last_7_days`
- `violation_rate_last_month`

### 3. Preprocessamento

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Normaliza√ß√£o
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Feature engineering
features['resource_ratio'] = features['required_cpu'] / features['available_cpu']
features['latency_margin'] = features['max_latency'] - features['current_latency']
```

### 4. Treinamento

**Script:** `apps/ml-nsmf/src/training/train.py`

```python
def train_model():
    # Carregar dados
    X_train, y_train = load_training_data()
    
    # Preprocessar
    X_train_scaled = preprocess(X_train)
    
    # Treinar modelo
    model.fit(X_train_scaled, y_train, epochs=50, validation_split=0.2)
    
    # Avaliar
    metrics = evaluate_model(model, X_test, y_test)
    
    # Salvar modelo
    model.save('data/models/model.h5')
    save_scaler(scaler, 'data/models/scaler.pkl')
```

### 5. Previs√£o

```python
def predict_viability(nest: dict, nasp_metrics: dict) -> dict:
    # Extrair features
    features = extract_features(nest, nasp_metrics)
    
    # Preprocessar
    features_scaled = scaler.transform([features])
    
    # Prever
    score = model.predict(features_scaled)[0][0]
    
    # Explicar (XAI)
    explanation = explainer.explain(features_scaled[0])
    
    return {
        'viability_score': float(score),
        'recommendation': 'ACCEPT' if score > threshold else 'REJECT',
        'explanation': explanation,
        'confidence': calculate_confidence(score)
    }
```

### 6. Explicabilidade (XAI)

**SHAP (SHapley Additive exPlanations):**

```python
import shap

explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(features)

# Visualiza√ß√£o
shap.summary_plot(shap_values, features)
```

**LIME (Local Interpretable Model-agnostic Explanations):**

```python
from lime import lime_tabular

explainer = lime_tabular.LimeTabularExplainer(
    training_data,
    feature_names=feature_names,
    mode='classification'
)
explanation = explainer.explain_instance(features, model.predict)
```

### 7. Interface Kafka I-03

**Producer (Enviar previs√£o):**

```python
from kafka import KafkaProducer
import json

producer = KafkaProducer(
    bootstrap_servers=['kafka:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

message = {
    'nest_id': nest_id,
    'viability_score': score,
    'recommendation': recommendation,
    'explanation': explanation,
    'timestamp': datetime.now().isoformat()
}

producer.send('ml-nsmf-predictions', value=message)
```

**Consumer (Receber NEST):**

```python
from kafka import KafkaConsumer

consumer = KafkaConsumer(
    'sem-csmf-nests',
    bootstrap_servers=['kafka:9092'],
    value_deserializer=lambda m: json.loads(m.decode('utf-8'))
)

for message in consumer:
    nest = message.value
    prediction = predict_viability(nest, get_nasp_metrics())
    send_to_decision_engine(prediction)
```

---

## üìä Persist√™ncia

### Armazenamento de Modelos

- **Modelo treinado:** `data/models/model.h5` (TensorFlow) ou `model.pkl` (scikit-learn)
- **Scaler:** `data/models/scaler.pkl`
- **Feature names:** `data/models/feature_names.json`
- **Metadata:** `data/models/metadata.json` (vers√£o, acur√°cia, data de treinamento)

### Datasets

- **Treinamento:** `data/datasets/training_data.csv`
- **Valida√ß√£o:** `data/datasets/validation_data.csv`
- **Teste:** `data/datasets/test_data.csv`

---

## üîç Observabilidade

### M√©tricas Prometheus

- `ml_nsmf_predictions_total` - Total de previs√µes realizadas
- `ml_nsmf_prediction_duration_seconds` - Tempo de previs√£o
- `ml_nsmf_model_accuracy` - Acur√°cia do modelo
- `ml_nsmf_viability_scores` - Histograma de scores
- `ml_nsmf_training_duration_seconds` - Tempo de treinamento

### Traces OTLP

- Trace completo: Recep√ß√£o NEST ‚Üí Previs√£o ‚Üí Envio ao Decision Engine
- Spans para cada etapa (preprocessamento, predi√ß√£o, explica√ß√£o)

---

## üß™ Testes

### Testes Unit√°rios

- Preprocessamento de features
- Predi√ß√£o do modelo
- Explicabilidade (XAI)
- Valida√ß√£o de inputs

### Testes de Integra√ß√£o

- Fluxo completo: NEST ‚Üí Previs√£o ‚Üí Kafka
- Comunica√ß√£o com NASP para m√©tricas
- Persist√™ncia de modelos

### Testes de Treinamento

- Valida√ß√£o cruzada
- M√©tricas de avalia√ß√£o (accuracy, precision, recall, F1)
- Overfitting detection

---

## üìù Exemplos

### Exemplo 1: Previs√£o de Viabilidade

**Input (NEST):**
```json
{
  "nestId": "nest-urllc-001",
  "sliceType": "URLLC",
  "requirements": {
    "latency": {"max": 10, "unit": "ms"},
    "reliability": 0.99999
  }
}
```

**Input (M√©tricas NASP):**
```json
{
  "cpu_utilization": 0.65,
  "memory_utilization": 0.70,
  "network_bandwidth_available": 500,
  "active_slices_count": 15
}
```

**Output:**
```json
{
  "nest_id": "nest-urllc-001",
  "viability_score": 0.87,
  "recommendation": "ACCEPT",
  "confidence": 0.92,
  "explanation": {
    "top_features": [
      {"feature": "latency_margin", "importance": 0.35},
      {"feature": "resource_ratio", "importance": 0.28},
      {"feature": "reliability_requirement", "importance": 0.22}
    ],
    "shap_values": {...}
  },
  "timestamp": "2025-01-19T10:30:00Z"
}
```

---

## ‚úÖ Crit√©rios de Sucesso

- ‚úÖ Modelo treinado com acur√°cia > 85%
- ‚úÖ Previs√£o em < 500ms
- ‚úÖ Explicabilidade (XAI) funcionando
- ‚úÖ Interface Kafka I-03 operacional
- ‚úÖ Retry logic implementado
- ‚úÖ Observabilidade completa
- ‚úÖ Testes passando (unit, integration, training)
- ‚úÖ Modelo versionado e persistido

---

## üöÄ Deploy

### Docker

```dockerfile
FROM python:3.12-slim

WORKDIR /app

# Instalar depend√™ncias
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copiar c√≥digo e modelo
COPY . .
COPY data/models/model.h5 data/models/

CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Kubernetes

- Deployment com 2 replicas
- Service para REST API (port 8000)
- ConfigMap para configura√ß√µes do modelo
- PersistentVolume para modelos e datasets

---

## üìö Refer√™ncias

- TensorFlow / Keras - Deep Learning Framework
- scikit-learn - Machine Learning Library
- SHAP - Explainable AI
- LIME - Local Interpretable Model-agnostic Explanations
- Apache Kafka - Distributed Streaming Platform

---

## ‚úî Pronto para implementa√ß√£o no Cursor

