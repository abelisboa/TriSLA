# Explainable AI (XAI) ‚Äî ML-NSMF

**Vers√£o:** S4.0  
**Data:** 2025-01-27  
**Origem do Conte√∫do:** `ML_NSMF_COMPLETE_GUIDE.md` (se√ß√£o Predi√ß√£o e XAI)

---

## üìã Sum√°rio

1. [Vis√£o Geral](#vis√£o-geral)
2. [M√©todos XAI](#m√©todos-xai)
3. [SHAP (Preferencial)](#shap-preferencial)
4. [LIME (Fallback)](#lime-fallback)
5. [Fallback](#fallback)
6. [Explica√ß√£o Gerada](#explica√ß√£o-gerada)
7. [Uso no ML-NSMF](#uso-no-ml-nsmf)

---

## Vis√£o Geral

O ML-NSMF utiliza Explainable AI (XAI) para fornecer explica√ß√µes transparentes das predi√ß√µes de viabilidade. As explica√ß√µes ajudam a entender quais features mais contribuem para a decis√£o e por que um SLA foi classificado como vi√°vel ou invi√°vel.

### Objetivos

1. **Transpar√™ncia**: Explicar como o modelo chegou √† predi√ß√£o
2. **Confian√ßa**: Aumentar confian√ßa nas decis√µes automatizadas
3. **Debugging**: Identificar problemas no modelo ou dados
4. **Compliance**: Atender requisitos de explicabilidade (GDPR, etc.)

### Hierarquia de M√©todos

1. **SHAP** (preferencial) ‚Äî Mais preciso e completo
2. **LIME** (fallback) ‚Äî Alternativa quando SHAP n√£o dispon√≠vel
3. **Fallback** ‚Äî Feature importance do modelo quando XAI n√£o dispon√≠vel

---

## M√©todos XAI

### SHAP (SHapley Additive exPlanations)

**Preferencial** ‚Äî M√©todo mais preciso e completo

**Vantagens:**
- Baseado em teoria de jogos (Shapley values)
- Explica√ß√µes consistentes e aditivas
- Funciona com qualquer modelo

**Desvantagens:**
- Pode ser lento para modelos grandes
- Requer mais recursos computacionais

### LIME (Local Interpretable Model-agnostic Explanations)

**Fallback** ‚Äî Alternativa quando SHAP n√£o dispon√≠vel

**Vantagens:**
- Mais r√°pido que SHAP
- Explica√ß√µes locais (vizinhan√ßa da predi√ß√£o)
- Funciona com qualquer modelo

**Desvantagens:**
- Explica√ß√µes podem variar entre execu√ß√µes
- Menos preciso que SHAP

### Fallback (Feature Importance)

**√öltimo recurso** ‚Äî Quando nem SHAP nem LIME dispon√≠veis

**Vantagens:**
- Sempre dispon√≠vel (vem do modelo)
- Muito r√°pido

**Desvantagens:**
- Explica√ß√µes globais (n√£o espec√≠ficas da predi√ß√£o)
- Menos preciso que SHAP/LIME

---

## SHAP (Preferencial)

### Implementa√ß√£o

```python
import shap
from predictor import RiskPredictor

predictor = RiskPredictor()

# Gerar explica√ß√£o SHAP
explanation = await predictor.explain_shap(
    prediction=prediction,
    features=normalized_features,
    model=model
)
```

### Resultado

```json
{
    "method": "SHAP",
    "features_importance": {
        "latency": 0.40,
        "throughput": 0.30,
        "packet_loss": 0.20,
        "jitter": 0.10,
        "cpu_utilization": 0.05,
        "memory_utilization": 0.03,
        "network_bandwidth_available": 0.02
    },
    "reasoning": "Risk level high devido principalmente a latency (import√¢ncia: 40.00%) e throughput (import√¢ncia: 30.00%). Requisitos de lat√™ncia e throughput s√£o muito restritivos para a infraestrutura atual.",
    "shap_available": true,
    "lime_available": false
}
```

### Shapley Values

SHAP calcula Shapley values para cada feature, representando a contribui√ß√£o m√©dia de cada feature para a predi√ß√£o:

- **Valores positivos**: Aumentam o score de viabilidade (reduzem risco)
- **Valores negativos**: Diminuem o score de viabilidade (aumentam risco)

### Visualiza√ß√£o

SHAP fornece visualiza√ß√µes √∫teis:
- **Summary plot**: Import√¢ncia de features
- **Waterfall plot**: Contribui√ß√£o de cada feature
- **Force plot**: Explica√ß√£o individual da predi√ß√£o

---

## LIME (Fallback)

### Implementa√ß√£o

```python
from lime.lime_tabular import LimeTabularExplainer
from predictor import RiskPredictor

predictor = RiskPredictor()

# Gerar explica√ß√£o LIME
explanation = await predictor.explain_lime(
    prediction=prediction,
    features=normalized_features,
    model=model
)
```

### Resultado

```json
{
    "method": "LIME",
    "features_importance": {
        "latency": 0.38,
        "throughput": 0.32,
        "packet_loss": 0.18,
        "jitter": 0.12
    },
    "reasoning": "Risk level high devido principalmente a latency e throughput. Requisitos s√£o muito restritivos.",
    "shap_available": false,
    "lime_available": true
}
```

### Explica√ß√£o Local

LIME gera explica√ß√µes locais (vizinhan√ßa da predi√ß√£o):
- Cria modelo interpret√°vel localmente
- Explica por que a predi√ß√£o foi feita para esta inst√¢ncia espec√≠fica
- Pode variar entre execu√ß√µes (aleatoriedade)

---

## Fallback

### Implementa√ß√£o

Quando nem SHAP nem LIME est√£o dispon√≠veis, o sistema usa feature importance do modelo:

```python
from predictor import RiskPredictor

predictor = RiskPredictor()

# Gerar explica√ß√£o fallback
explanation = await predictor.explain_fallback(
    prediction=prediction,
    model=model
)
```

### Resultado

```json
{
    "method": "fallback",
    "features_importance": {
        "latency": 0.4,
        "throughput": 0.3,
        "packet_loss": 0.2,
        "jitter": 0.1
    },
    "reasoning": "Risk level high devido principalmente √† lat√™ncia (import√¢ncia: 40%) e throughput (import√¢ncia: 30%)."
}
```

### Feature Importance

Feature importance vem diretamente do modelo Random Forest:
- Calculada durante o treinamento
- Representa import√¢ncia global (n√£o espec√≠fica da predi√ß√£o)
- Sempre dispon√≠vel

---

## Explica√ß√£o Gerada

### Estrutura da Explica√ß√£o

Toda explica√ß√£o XAI inclui:

1. **M√©todo usado**: SHAP, LIME ou fallback
2. **Feature importance**: Ranking de import√¢ncia de features
3. **Reasoning textual**: Explica√ß√£o em linguagem natural
4. **Disponibilidade**: Status de SHAP e LIME

### Exemplo Completo

```json
{
    "prediction_id": "pred-001",
    "nest_id": "nest-urllc-001",
    "viability_score": 0.75,
    "risk_level": "high",
    "confidence": 0.85,
    "recommendation": "REJECT",
    "xai_explanation": {
        "method": "SHAP",
        "features_importance": {
            "latency": 0.40,
            "throughput": 0.30,
            "packet_loss": 0.20,
            "jitter": 0.10,
            "cpu_utilization": 0.05,
            "memory_utilization": 0.03,
            "network_bandwidth_available": 0.02
        },
        "reasoning": "Risk level high devido principalmente a latency (import√¢ncia: 40.00%) e throughput (import√¢ncia: 30.00%). Requisitos de lat√™ncia e throughput s√£o muito restritivos para a infraestrutura atual. CPU e mem√≥ria est√£o em n√≠veis aceit√°veis, mas bandwidth dispon√≠vel √© limitado.",
        "shap_available": true,
        "lime_available": false
    },
    "timestamp": "2025-01-27T10:00:00Z"
}
```

### Reasoning Textual

O reasoning textual √© gerado automaticamente com base na feature importance:

- **Alta import√¢ncia (> 0.3)**: Mencionada explicitamente
- **M√©dia import√¢ncia (0.1-0.3)**: Mencionada se relevante
- **Baixa import√¢ncia (< 0.1)**: Omitida ou mencionada brevemente

---

## Uso no ML-NSMF

### Integra√ß√£o no Pipeline

1. **Predi√ß√£o**: Modelo gera score de viabilidade
2. **XAI**: Sistema gera explica√ß√£o (SHAP/LIME/fallback)
3. **Envio**: Predi√ß√£o + explica√ß√£o enviada ao Decision Engine (I-03)

### C√≥digo de Exemplo

```python
from predictor import RiskPredictor

predictor = RiskPredictor()

# Predi√ß√£o
prediction = await predictor.predict(normalized_features)

# Explica√ß√£o XAI
explanation = await predictor.explain(
    prediction=prediction,
    features=normalized_features,
    model=model
)

# Combinar predi√ß√£o e explica√ß√£o
result = {
    "prediction": prediction,
    "explanation": explanation
}

# Enviar ao Decision Engine
await producer.send_prediction(result)
```

### Performance

- **SHAP**: ~200-500ms por predi√ß√£o
- **LIME**: ~100-300ms por predi√ß√£o
- **Fallback**: < 10ms (instant√¢neo)

### Configura√ß√£o

```bash
# Habilitar/desabilitar XAI
XAI_ENABLED=true
XAI_METHOD=SHAP  # SHAP, LIME, ou AUTO
XAI_TIMEOUT=500  # ms
```

---

## Origem do Conte√∫do

Este documento foi consolidado a partir de:
- `ML_NSMF_COMPLETE_GUIDE.md` ‚Äî Se√ß√£o "Predi√ß√£o e XAI"

**√öltima atualiza√ß√£o:** 2025-01-27  
**Vers√£o:** S4.0

