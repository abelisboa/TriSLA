# Guia Completo of M√≥dulo ML-NSMF

**Vers√£o:** 3.5.0  
**Data:** 2025-01-27  
**M√≥dulo:** Machine Learning Network Slice Management Function

---

## üìã Sum√°rio

1. [Vis√£o Geral](#vis√£o-geral)
2. [Arquitetura of M√≥dulo](#arquitetura-do-m√≥dulo)
3. [Funcionamento of M√≥dulo](#funcionamento-do-m√≥dulo)
4. [Treinamento of Modelo](#treinamento-do-modelo)
5. [Predi√ß√£o e XAI](#predi√ß√£o-e-xai)
6. [Integra√ß√£o com Outros M√≥dulos](#integra√ß√£o-com-outros-m√≥dulos)
7. [Interface I-03 (Kafka)](#interface-i-03-kafka)
8. [Observabilidade](#observabilidade)
9. [Exemplos de Uso](#exemplos-de-uso)
10. [Troubleshooting](#troubleshooting)

---

## üéØ Vis√£o Geral

O **ML-NSMF (Machine Learning Network Slice Management Function)** √© respons√°vel por prever a viabilidade de aceita√ß√£o de SLAs baseado in m√©tricas hist√≥ricas, caracter√≠sticas of NEST e estado atual dos recursos of infraestrutura.

### Objetivos

1. **Predi√ß√£o de Viabilidade:** Prever se um SLA pode ser atendido (score 0-1)
2. **Explicabilidade (XAI):** Fornecer explica√ß√µes das predi√ß√µes usando SHAP e LIME
3. **Recomenda√ß√µes:** Sugerir ajustes de requisitos quando necess√°rio
4. **Integra√ß√£o:** Comunicar-se com Decision Engine via interface I-03 (Kafka)

### Caracter√≠sticas Principais

- **Modelo ML:** Random Forest (atual) ou LSTM/GRU (futuro)
- **XAI:** SHAP e LIME for explica√ß√µes
- **Tempo de Resposta:** < 500ms
- **Acur√°cia:** > 85% (modelo treinado)

---

## üèóÔ∏è Arquitetura of M√≥dulo

### Estrutura de Diret√≥rios

```
apps/ml-nsmf/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main.py                 # Aplica√ß√£o FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ predictor.py            # Classe RiskPredictor (predi√ß√£o)
‚îÇ   ‚îú‚îÄ‚îÄ kafka_consumer.py       # Consumer Kafka (recebe NESTs)
‚îÇ   ‚îú‚îÄ‚îÄ kafka_producer.py       # Producer Kafka (envia predi√ß√µes)
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ viability_model.pkl    # Modelo treinado (Random Forest)
‚îÇ   ‚îú‚îÄ‚îÄ scaler.pkl              # Scaler for normaliza√ß√£o
‚îÇ   ‚îî‚îÄ‚îÄ model_metadata.json     # Metadados of modelo
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ datasets/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ trisla_ml_dataset.csv  # Dataset de treinamento
‚îÇ   ‚îî‚îÄ‚îÄ training/               # Scripts de treinamento
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ unit/                   # Testes unit√°rios
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

### Componentes Principais

1. **RiskPredictor** ‚Äî Classe principal for predi√ß√£o
2. **MetricsConsumer** ‚Äî Consome m√©tricas of NASP via Kafka
3. **PredictionProducer** ‚Äî Envia predi√ß√µes ao Decision Engine via Kafka
4. **Modelo ML** ‚Äî Modelo treinado (Random Forest ou LSTM/GRU)
5. **XAI Explainer** ‚Äî Explicador usando SHAP/LIME

---

## ‚öôÔ∏è Funcionamento of M√≥dulo

### Pipeline de Processamento

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Recebe NEST    ‚îÇ  (via Kafka I-02)
‚îÇ  of SEM-CSMF    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Coleta M√©tricas‚îÇ  (do NASP via NASP Adapter)
‚îÇ  Atuais         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Extrai Features‚îÇ  (do NEST + m√©tricas)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Normaliza      ‚îÇ  (usando scaler treinado)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Predi√ß√£o ML    ‚îÇ  (modelo treinado)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Explica√ß√£o XAI ‚îÇ  (SHAP/LIME)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Envia ao       ‚îÇ  (via Kafka I-03)
‚îÇ  Decision Engine‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Fluxo Detalhado

1. **Recep√ß√£o de NEST**
   - Consumer Kafka recebe NEST of SEM-CSMF
   - T√≥pico: `sem-csmf-nests`

2. **Coleta de M√©tricas**
   - Consulta NASP Adapter for m√©tricas atuais
   - Dom√≠nios: RAN, Transport, Core

3. **Extra√ß√£o de Features**
   - Do NEST: `sliceType`, `latency_requirement`, `throughput_requirement`, `reliability_requirement`
   - Das m√©tricas: `cpu_utilization`, `memory_utilization`, `network_bandwidth_available`, `active_slices_count`
   - Feature engineering: `latency_throughput_ratio`, `reliability_packet_loss_ratio`, etc.

4. **Normaliza√ß√£o**
   - Usa `scaler.pkl` treinado
   - Normaliza√ß√£o StandardScaler ou MinMaxScaler

5. **Predi√ß√£o**
   - Modelo ML gera score de viabilidade (0-1)
   - Threshold configur√°vel (ex: 0.7)

6. **Explica√ß√£o (XAI)**
   - SHAP ou LIME gera explica√ß√£o
   - Feature importance ranking
   - Reasoning textual

7. **Envio ao Decision Engine**
   - Producer Kafka envia predi√ß√£o
   - T√≥pico: `ml-nsmf-predictions`

---

## üéì Treinamento of Modelo

### 1. Prepara√ß√£o dos Dados

#### Dataset de Treinamento

**Arquivo:** `apps/ml-nsmf/data/datasets/trisla_ml_dataset.csv`

**Estrutura of Dataset:**

| Coluna | Tipo | Descri√ß√£o |
|--------|------|-----------|
| `latency` | float | Lat√™ncia requerida (ms) |
| `throughput` | float | Throughput requerido (Mbps) |
| `reliability` | float | Confiabilidade requerida (0-1) |
| `jitter` | float | Jitter requerido (ms) |
| `packet_loss` | float | Perda de pacotes (0-1) |
| `cpu_utilization` | float | Utiliza√ß√£o de CPU (0-1) |
| `memory_utilization` | float | Utiliza√ß√£o de mem√≥ria (0-1) |
| `network_bandwidth_available` | float | Largura de banda dispon√≠vel (Mbps) |
| `active_slices_count` | int | N√∫mero de slices ativos |
| `slice_type_encoded` | int | Tipo de slice codificado (1=eMBB, 2=URLLC, 3=mMTC) |
| `viability_score` | float | Score de viabilidade (0-1) - **TARGET** |

**Feature Engineering:**

```python
# Features derivadas
features['latency_throughput_ratio'] = features['latency'] / features['throughput']
features['reliability_packet_loss_ratio'] = features['reliability'] / (features['packet_loss'] + 0.001)
features['jitter_latency_ratio'] = features['jitter'] / (features['latency'] + 0.001)
features['resource_ratio'] = features['required_cpu'] / features['available_cpu']
```

### 2. Script de Treinamento

**Arquivo:** `apps/ml-nsmf/training/train_model.py` (a ser criado)

```python
"""
Script de Treinamento of Modelo ML-NSMF
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import pickle
import json
from datetime import datetime
import os

# Carregar dataset
def load_dataset(path: str) -> pd.DataFrame:
    """Carrega dataset de treinamento"""
    df = pd.read_csv(path)
    return df

# Feature engineering
def engineer_features(df: pd.DataFrame) -> pd.DataFrame:
    """Cria features derivadas"""
    df['latency_throughput_ratio'] = df['latency'] / (df['throughput'] + 0.001)
    df['reliability_packet_loss_ratio'] = df['reliability'] / (df['packet_loss'] + 0.001)
    df['jitter_latency_ratio'] = df['jitter'] / (df['latency'] + 0.001)
    return df

# Treinar modelo
def train_model(X_train, y_train):
    """Treina modelo Random Forest"""
    model = RandomForestRegressor(
        n_estimators=100,
        max_depth=10,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1
    )
    
    model.fit(X_train, y_train)
    return model

# Avaliar modelo
def evaluate_model(model, X_test, y_test):
    """Avalia modelo"""
    y_pred = model.predict(X_test)
    
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    return {
        "mse": mse,
        "mae": mae,
        "r2": r2
    }

# Fun√ß√£o principal
def main():
    # 1. Carregar dataset
    dataset_path = "data/datasets/trisla_ml_dataset.csv"
    df = load_dataset(dataset_path)
    
    # 2. Feature engineering
    df = engineer_features(df)
    
    # 3. Separar features e target
    feature_columns = [
        "latency", "throughput", "reliability", "jitter", "packet_loss",
        "cpu_utilization", "memory_utilization", "network_bandwidth_available",
        "active_slices_count", "slice_type_encoded",
        "latency_throughput_ratio", "reliability_packet_loss_ratio",
        "jitter_latency_ratio"
    ]
    
    X = df[feature_columns]
    y = df['viability_score']
    
    # 4. Split train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # 5. Normaliza√ß√£o
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # 6. Treinar modelo
    model = train_model(X_train_scaled, y_train)
    
    # 7. Avaliar modelo
    train_metrics = evaluate_model(model, X_train_scaled, y_train)
    test_metrics = evaluate_model(model, X_test_scaled, y_test)
    
    # 8. Cross-validation
    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')
    
    # 9. Feature importance
    feature_importance = dict(zip(feature_columns, model.feature_importances_))
    
    # 10. Salvar modelo
    os.makedirs("models", exist_ok=True)
    
    # Salvar modelo
    with open("models/viability_model.pkl", "wb") as f:
        pickle.dump(model, f)
    
    # Salvar scaler
    with open("models/scaler.pkl", "wb") as f:
        pickle.dump(scaler, f)
    
    # Salvar metadados
    metadata = {
        "model_type": "random_forest",
        "feature_columns": feature_columns,
        "training_history": {
            "model_type": "random_forest",
            "train_samples": len(X_train),
            "test_samples": len(X_test),
            "train_mse": train_metrics["mse"],
            "test_mse": test_metrics["mse"],
            "train_mae": train_metrics["mae"],
            "test_mae": test_metrics["mae"],
            "train_r2": train_metrics["r2"],
            "test_r2": test_metrics["r2"],
            "cv_mean": cv_scores.mean(),
            "cv_std": cv_scores.std(),
            "feature_importance": feature_importance,
            "timestamp": datetime.utcnow().isoformat() + "Z"
        },
        "model_path": "viability_model.pkl",
        "scaler_path": "scaler.pkl"
    }
    
    with open("models/model_metadata.json", "w") as f:
        json.dump(metadata, f, indent=2)
    
    print("Modelo treinado e salvo com sucesso!")
    print(f"Test R¬≤: {test_metrics['r2']:.4f}")
    print(f"Test MAE: {test_metrics['mae']:.4f}")

if __name__ == "__main__":
    main()
```

### 3. Executar Treinamento

**Comando:**
```bash
cd apps/ml-nsmf
python training/train_model.py
```

**Sa√≠da Esperada:**
```
Modelo treinado e salvo com sucesso!
Test R¬≤: 0.9028
Test MAE: 0.0464
```

### 4. Valida√ß√£o of Modelo

**M√©tricas de Avalia√ß√£o:**

- **R¬≤ Score:** > 0.85 (objetivo)
- **MAE (Mean Absolute Error):** < 0.05
- **MSE (Mean Squared Error):** < 0.01
- **Cross-Validation:** CV score > 0.85

**Feature Importance:**

O modelo calcula import√¢ncia de features automaticamente. Exemplo:

```json
{
  "reliability": 0.370,
  "latency_throughput_ratio": 0.254,
  "latency": 0.130,
  "throughput": 0.089,
  ...
}
```

### 5. Retreinamento

**Quando Retreinar:**

1. **Novos dados dispon√≠veis:** Acumular novos exemplos
2. **Degrada√ß√£o de performance:** R¬≤ < 0.80
3. **Mudan√ßas no ambiente:** Novos tipos de slice, mudan√ßas na infraestrutura
4. **Per√≠odo regular:** Mensal ou trimestral

**Processo de Retreinamento:**

1. Coletar novos dados of NASP
2. Adicionar ao dataset existente
3. Executar script de treinamento
4. Validar novo modelo
5. Se melhor, substituir modelo antigo
6. Se pior, manter modelo atual

---

## üîÆ Predi√ß√£o e XAI

### 1. Predi√ß√£o de Viabilidade

**Classe:** `RiskPredictor`

**M√©todo:** `predict()`

```python
from predictor import RiskPredictor
import numpy as np

predictor = RiskPredictor()

# M√©tricas normalizadas
normalized_metrics = np.array([0.15, 0.5, 0.001, 0.2])

# Predi√ß√£o
prediction = await predictor.predict(normalized_metrics)

# Resultado
{
    "risk_score": 0.75,
    "risk_level": "high",
    "confidence": 0.85,
    "timestamp": "2025-01-27T10:00:00Z"
}
```

**Interpreta√ß√£o of Score:**

- **0.0 - 0.4:** Baixo risco (ACCEPT)
- **0.4 - 0.7:** Risco m√©dio (CONDITIONAL_ACCEPT)
- **0.7 - 1.0:** Alto risco (REJECT)

### 2. Explicabilidade (XAI)

**M√©todo:** `explain()`

**SHAP (SHapley Additive exPlanations):**

```python
explanation = await predictor.explain(prediction, normalized_metrics, model)

# Resultado
{
    "method": "SHAP",
    "features_importance": {
        "latency": 0.40,
        "throughput": 0.30,
        "packet_loss": 0.20,
        "jitter": 0.10
    },
    "reasoning": "Risk level high devido principalmente a latency (import√¢ncia: 40.00%)",
    "shap_available": True,
    "lime_available": False
}
```

**LIME (Local Interpretable Model-agnostic Explanations):**

Se SHAP n√£o estiver dispon√≠vel, usa LIME:

```python
{
    "method": "LIME",
    "features_importance": {...},
    "reasoning": "...",
    "shap_available": False,
    "lime_available": True
}
```

**Fallback:**

Se nem SHAP nem LIME estiverem dispon√≠veis:

```python
{
    "method": "fallback",
    "features_importance": {
        "latency": 0.4,
        "throughput": 0.3,
        "packet_loss": 0.2,
        "jitter": 0.1
    },
    "reasoning": "Risk level high devido principalmente √† lat√™ncia"
}
```

---

## üîó Integra√ß√£o com Outros M√≥dulos

### 1. SEM-CSMF (Interface I-02)

**Tipo:** Kafka Consumer  
**T√≥pico:** `sem-csmf-nests`  
**Payload:** NEST (Network Slice Template)

**C√≥digo:**
```python
from kafka_consumer import MetricsConsumer

consumer = MetricsConsumer()

# Consumir NESTs
for message in consumer.consume_nests():
    nest = message.value
    # Processar NEST
    prediction = await predictor.predict_from_nest(nest)
```

### 2. Decision Engine (Interface I-03)

**Tipo:** Kafka Producer  
**T√≥pico:** `ml-nsmf-predictions`  
**Payload:** Predi√ß√£o + Explica√ß√£o

**C√≥digo:**
```python
from kafka_producer import PredictionProducer

producer = PredictionProducer()

# Enviar predi√ß√£o
await producer.send_prediction(prediction, explanation)
```

### 3. NASP Adapter

**Tipo:** HTTP REST  
**Endpoint:** `http://nasp-adapter:8080/api/v1/metrics`

**C√≥digo:**
```python
import httpx

async with httpx.AsyncClient() as client:
    response = await client.get("http://nasp-adapter:8080/api/v1/metrics")
    metrics = response.json()
```

---

## üì° Interface I-03 (Kafka)

### T√≥pico Kafka

**Nome:** `ml-nsmf-predictions`

### Schema of Mensagem

```json
{
  "nest_id": "nest-001",
  "intent_id": "intent-001",
  "viability_score": 0.75,
  "risk_level": "high",
  "confidence": 0.85,
  "explanation": {
    "method": "SHAP",
    "features_importance": {
      "latency": 0.40,
      "throughput": 0.30,
      "packet_loss": 0.20,
      "jitter": 0.10
    },
    "reasoning": "Risk level high devido principalmente a latency"
  },
  "timestamp": "2025-01-27T10:00:00Z"
}
```

### Producer Kafka

**Arquivo:** `apps/ml-nsmf/src/kafka_producer.py`

```python
from kafka import KafkaProducer
import json

producer = KafkaProducer(
    bootstrap_servers=['kafka:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

# Enviar predi√ß√£o
producer.send('ml-nsmf-predictions', value=prediction_data)
```

---

## üìä Observabilidade

### M√©tricas Prometheus

| M√©trica | Tipo | Descri√ß√£o |
|---------|------|-----------|
| `ml_nsmf_predictions_total` | Counter | Total de predi√ß√µes realizadas |
| `ml_nsmf_prediction_duration_seconds` | Histogram | Tempo de predi√ß√£o |
| `ml_nsmf_model_accuracy` | Gauge | Acur√°cia of modelo |
| `ml_nsmf_viability_scores` | Histogram | Distribui√ß√£o de scores |
| `ml_nsmf_training_duration_seconds` | Histogram | Tempo de treinamento |

### Traces OTLP

**Spans:**
- `predict_risk` ‚Äî Predi√ß√£o completa
- `normalize_metrics` ‚Äî Normaliza√ß√£o
- `explain_prediction` ‚Äî Explica√ß√£o XAI
- `send_prediction` ‚Äî Envio ao Decision Engine

---

## üí° Exemplos de Uso

### Exemplo 1: Predi√ß√£o Simples

```python
from predictor import RiskPredictor

predictor = RiskPredictor()

# M√©tricas of NEST
metrics = {
    "latency": 15.0,
    "throughput": 500.0,
    "packet_loss": 0.001,
    "jitter": 2.0
}

# Normalizar
normalized = await predictor.normalize(metrics)

# Predizer
prediction = await predictor.predict(normalized)

print(f"Score: {prediction['risk_score']}")
print(f"Level: {prediction['risk_level']}")
```

### Exemplo 2: Predi√ß√£o com Explica√ß√£o

```python
# Predi√ß√£o
prediction = await predictor.predict(normalized)

# Explica√ß√£o
explanation = await predictor.explain(prediction, normalized)

print(f"Method: {explanation['method']}")
print(f"Top Feature: {max(explanation['features_importance'].items(), key=lambda x: x[1])}")
print(f"Reasoning: {explanation['reasoning']}")
```

### Exemplo 3: Treinamento of Modelo

```bash
# 1. Preparar dataset
python scripts/prepare_dataset.py

# 2. Treinar modelo
cd apps/ml-nsmf
python training/train_model.py

# 3. Validar modelo
python scripts/validate_model.py

# 4. Deploy modelo
cp models/viability_model.pkl /path/to/production/models/
cp models/scaler.pkl /path/to/production/models/
```

---

## üîß Troubleshooting

### Problema 1: Modelo n√£o carrega

**Sintoma:** `FileNotFoundError: models/viability_model.pkl`

**solution:**
```bash
# Verificar se modelo existe
ls -la apps/ml-nsmf/models/

# Se n√£o existir, treinar modelo
cd apps/ml-nsmf
python training/train_model.py
```

### Problema 2: SHAP/LIME n√£o dispon√≠vel

**Sintoma:** `ImportError: No module named 'shap'`

**solution:**
```bash
pip install shap==0.43.0 lime==0.2.0.1
```

### Problema 3: Predi√ß√£o muito lenta

**Sintoma:** Tempo de predi√ß√£o > 500ms

**Solu√ß√µes:**
1. Otimizar modelo (reduzir n√∫mero de √°rvores)
2. Usar modelo mais simples (Linear Regression)
3. Cache de predi√ß√µes similares

### Problema 4: Acur√°cia baixa

**Sintoma:** R¬≤ < 0.80

**Solu√ß√µes:**
1. Coletar mais dados de treinamento
2. Feature engineering adicional
3. Ajustar hiperpar√¢metros of modelo
4. Tentar modelo diferente (XGBoost, Neural Network)

---

## üìö Refer√™ncias

- **scikit-learn:** https://scikit-learn.org/
- **SHAP:** https://shap.readthedocs.io/
- **LIME:** https://github.com/marcotcr/lime
- **Kafka Python:** https://kafka-python.readthedocs.io/
- **Random Forest:** https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html

---

## üéØ Conclus√£o

O ML-NSMF fornece predi√ß√µes de viabilidade de SLA com explica√ß√µes usando XAI. O m√≥dulo:

- ‚úÖ **Prediz viabilidade** de SLAs baseado in m√©tricas
- ‚úÖ **Explica predi√ß√µes** usando SHAP/LIME
- ‚úÖ **Integra-se** com SEM-CSMF e Decision Engine
- ‚úÖ **Observ√°vel** via Prometheus e OpenTelemetry
- ‚úÖ **Trein√°vel** com novos dados

Para mais informa√ß√µes, consulte:
- `apps/ml-nsmf/src/predictor.py` ‚Äî Classe principal
- `apps/ml-nsmf/models/model_metadata.json` ‚Äî Metadados of modelo
- `apps/ml-nsmf/README.md` ‚Äî README of m√≥dulo

---

**Fim of Guia**

