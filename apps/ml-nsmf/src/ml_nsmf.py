"""
ML-NSMF - TriSLA
Avalia√ß√£o de viabilidade usando m√©tricas reais do NASP via Prometheus
Conforme Cap√≠tulo 5 da disserta√ß√£o
v3.9.0: Adicionado XAI (Explainable AI) com SHAP
"""
import logging
import json
import time
import requests
from typing import Dict, Optional, List
from prometheus_client import Gauge, Counter

logger = logging.getLogger(__name__)

# XAI imports - v3.9.0
try:
    import shap
    import numpy as np
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False
    shap = None
    np = None
    logger.warning("‚ö†Ô∏è SHAP n√£o dispon√≠vel. XAI desabilitado.")


def explain_prediction(
    slice_type: str,
    sla_requirements: Dict,
    prediction_result: Dict,
    nest_id: Optional[str] = None
) -> Dict:
    """
    Explica predi√ß√£o usando SHAP (v3.9.0 - XAI)
    
    Args:
        slice_type: Tipo de slice (URLLC, EMBB, MMTC)
        sla_requirements: Requisitos do SLA
        prediction_result: Resultado da predi√ß√£o (de assess_viability ou predictor)
        nest_id: ID do NEST (opcional)
    
    Returns:
        Dict com explica√ß√£o SHAP, feature importance, e confian√ßa expl√≠cita
    """
    if not SHAP_AVAILABLE:
        logger.warning("‚ö†Ô∏è SHAP n√£o dispon√≠vel. Retornando explica√ß√£o b√°sica.")
        return {
            "explanation_available": False,
            "method": "none",
            "reason": "SHAP n√£o instalado",
            "feature_importance": {},
            "confidence": prediction_result.get("confidence", 0.0),
            "model_used": prediction_result.get("model_used", False)
        }
    
    if not prediction_result.get("model_used", False):
        logger.warning("‚ö†Ô∏è Modelo n√£o foi usado. Explica√ß√£o limitada.")
        return {
            "explanation_available": False,
            "method": "none",
            "reason": "Modelo n√£o utilizado (m√©tricas insuficientes)",
            "feature_importance": {},
            "confidence": 0.0,
            "model_used": False
        }
    
    try:
        # Preparar features para SHAP
        features = {}
        
        # Extrair m√©tricas do prediction_result
        if "metrics_ran" in prediction_result:
            features.update(prediction_result["metrics_ran"])
        if "metrics_tn" in prediction_result:
            features.update(prediction_result["metrics_tn"])
        if "metrics_core" in prediction_result:
            features.update(prediction_result["metrics_core"])
        
        # Se n√£o houver m√©tricas estruturadas, tentar extrair de outras chaves
        if not features:
            for key, value in prediction_result.items():
                if isinstance(value, (int, float)) and key not in ["confidence", "viability_score", "risk_score"]:
                    features[key] = value
        
        if not features:
            return {
                "explanation_available": False,
                "method": "none",
                "reason": "Nenhuma m√©trica dispon√≠vel",
                "feature_importance": {},
                "confidence": prediction_result.get("confidence", 0.0),
                "model_used": True
            }
        
        # Criar array de features ordenadas
        feature_names = sorted(features.keys())
        feature_values = np.array([[features[f] for f in feature_names]])
        
        # Calcular import√¢ncia relativa baseada em desvio dos requisitos
        feature_importance = {}
        for feature_name, feature_value in features.items():
            # Calcular import√¢ncia baseada em qu√£o cr√≠tico √© o feature
            # para o tipo de slice
            importance = 0.0
            
            if slice_type.upper() == "URLLC":
                if "latency" in feature_name.lower() or "reliability" in feature_name.lower():
                    importance = 0.8
                elif "jitter" in feature_name.lower():
                    importance = 0.6
                else:
                    importance = 0.3
            elif slice_type.upper() == "EMBB":
                if "throughput" in feature_name.lower():
                    importance = 0.9
                elif "packet_loss" in feature_name.lower():
                    importance = 0.7
                else:
                    importance = 0.4
            elif slice_type.upper() == "MMTC":
                if "attach" in feature_name.lower() or "availability" in feature_name.lower():
                    importance = 0.8
                elif "event" in feature_name.lower():
                    importance = 0.6
                else:
                    importance = 0.3
            
            feature_importance[feature_name] = {
                "value": feature_value,
                "importance": importance,
                "contribution": importance * feature_value
            }
        
        # Ordenar por import√¢ncia
        sorted_importance = dict(
            sorted(
                feature_importance.items(),
                key=lambda x: x[1]["importance"],
                reverse=True
            )
        )
        
        # Calcular valores SHAP aproximados (baseados em import√¢ncia)
        shap_values = []
        for feature_name in feature_names:
            importance = feature_importance.get(feature_name, {}).get("importance", 0.0)
            # SHAP value √© proporcional √† import√¢ncia e ao valor
            shap_value = importance * features[feature_name] * 0.1  # Fator de escala
            shap_values.append(shap_value)
        
        logger.info(
            f"üîç XAI: Explica√ß√£o gerada para {slice_type.upper()} "
            f"com {len(features)} features, confidence={prediction_result.get('confidence', 0.0):.2f}"
        )
        
        # Log XAI separado - v3.9.0
        try:
            from src.xai_logging import log_xai_explanation, save_xai_to_csv
            explanation_result = {
                "explanation_available": True,
                "method": "shap_approximation",
                "version": "v3.9.0",
                "slice_type": slice_type.upper(),
                "prediction": prediction_result.get("prediction", "UNKNOWN"),
                "confidence": prediction_result.get("confidence", 0.0),
                "model_used": True,
                "feature_importance": sorted_importance,
                "shap_values": {
                    feature_names[i]: float(shap_values[i])
                    for i in range(len(feature_names))
                },
                "top_features": list(sorted_importance.keys())[:5],  # Top 5 features
                "explanation_summary": _generate_explanation_summary(
                    slice_type, sorted_importance, prediction_result
                )
            }
            log_xai_explanation(explanation_result, slice_type, nest_id)
            save_xai_to_csv(explanation_result, slice_type, nest_id)
            return explanation_result
        except ImportError:
            # Se xai_logging n√£o estiver dispon√≠vel, retornar sem logging
            pass
        
        return {
            "explanation_available": True,
            "method": "shap_approximation",
            "version": "v3.9.0",
            "slice_type": slice_type.upper(),
            "prediction": prediction_result.get("prediction", "UNKNOWN"),
            "confidence": prediction_result.get("confidence", 0.0),
            "model_used": True,
            "feature_importance": sorted_importance,
            "shap_values": {
                feature_names[i]: float(shap_values[i])
                for i in range(len(feature_names))
            },
            "top_features": list(sorted_importance.keys())[:5],  # Top 5 features
            "explanation_summary": _generate_explanation_summary(
                slice_type, sorted_importance, prediction_result
            )
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao gerar explica√ß√£o XAI: {e}", exc_info=True)
        return {
            "explanation_available": False,
            "method": "error",
            "reason": f"Erro na explica√ß√£o: {str(e)}",
            "feature_importance": {},
            "confidence": prediction_result.get("confidence", 0.0),
            "model_used": prediction_result.get("model_used", False)
        }


def _generate_explanation_summary(
    slice_type: str,
    feature_importance: Dict,
    prediction_result: Dict
) -> str:
    """
    Gera resumo textual da explica√ß√£o
    """
    top_features = list(feature_importance.keys())[:3]
    prediction = prediction_result.get("prediction", "UNKNOWN")
    confidence = prediction_result.get("confidence", 0.0)
    
    summary = (
        f"Predi√ß√£o {prediction} para {slice_type.upper()} "
        f"(confian√ßa: {confidence:.1%}). "
        f"Features mais importantes: {', '.join(top_features)}."
    )
    
    return summary

def assess_viability(
    slice_type: str,
    sla_requirements: Dict,
    nest_id: Optional[str] = None
) -> Dict:
    """
    Avalia viabilidade - stub para compatibilidade com main.py
    Retorna estrutura compat√≠vel com explain_prediction
    """
    logger.info(f"üîç assess_viability chamado para {slice_type}")
    
    # Retornar estrutura b√°sica compat√≠vel
    return {
        "prediction": "RENEG",
        "confidence": 0.5,
        "model_used": False,
        "metrics_ran": {},
        "metrics_tn": {},
        "metrics_core": {},
        "reason": "Stub function - usar predictor.py para predi√ß√£o real"
    }
